{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Natural Language Processing Tutorial Level Intermediate - NLP102.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielosullivan2007/Teaching/blob/master/Natural_Language_Processing_Tutorial_Level_Intermediate_NLP102.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wypwfksmUq_B"
      },
      "source": [
        "#  <span style=\"color:orange\">Natural Language Processing Tutorial (NLP102) - Level Intermediate</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAVzw3DvUq_D"
      },
      "source": [
        "**Date Updated: Feb 25, 2020**\n",
        "\n",
        "# 1.0 Objective of Tutorial\n",
        "Welcome to Natural Language Processing Tutorial (NLP102). This tutorial assumes that you have completed __[Natural Language Processing Tutorial (NLP101) - Level Beginner](https://github.com/pycaret/pycaret/blob/master/Tutorials/Natural%20Language%20Processing%20Tutorial%20Level%20Beginner%20-%20NLP101.ipynb)__. If you haven't we strongly recommend you to go back and progress through the beginner's tutorial as several key concepts that we aim to cover in this tutorial are inter-connected with Beginner's Tutorial.\n",
        "\n",
        "Building on the previous tutorial, we will learn the following in this tutorial: \n",
        "\n",
        "* **Custom Stopwords:**  How to define custom stopwords?\n",
        "* **Evaluate Topic Model:**  How to evaluate performance of a topic model?\n",
        "* **Hyperparameter Tuning:**  How to tune hyperparameter (# of topics) for a topic model?\n",
        "* **Save / Load Experiment:**  How to save an entire experiment?\n",
        "\n",
        "Read Time : Approx. 30 Minutes\n",
        "\n",
        "\n",
        "## 1.1 Installing PyCaret\n",
        "If you haven't installed PyCaret yet. Please follow the link to __[Beginner's Tutorial](https://github.com/pycaret/pycaret/blob/master/Tutorials/Natural%20Language%20Processing%20Tutorial%20Level%20Beginner%20-%20NLP101.ipynb)__ for instruction on how to install pycaret.\n",
        "\n",
        "\n",
        "## 1.2 Pre-Requisites\n",
        "- Python 3.x\n",
        "- Latest version of pycaret\n",
        "- Internet connection to load data from pycaret's repository\n",
        "- Completion of Natural Language Processing Tutorial (NLP101) - Level Beginner\n",
        "- Completion of Natural Binary Classification Tutorial (CLF101) - Level Beginner\n",
        "\n",
        "## 1.3 For Google colab users:\n",
        "If you are running this notebook on Google colab, below code of cells must be run at top of the notebook to display interactive visuals.<br/>\n",
        "<br/>\n",
        "`from pycaret.utils import enable_colab` <br/>\n",
        "`enable_colab()`\n",
        "\n",
        "## 1.4 See also:\n",
        "- __[Natural Language Processing Tutorial (NLP101) - Level Beginner](https://github.com/pycaret/pycaret/blob/master/Tutorials/Natural%20Language%20Processing%20Tutorial%20Level%20Beginner%20-%20NLP101.ipynb)__\n",
        "- __[Natural Language Processing Tutorial (NLP103) - Level Expert](https://github.com/pycaret/pycaret/blob/master/Tutorials/Natural%20Language%20Processing%20Tutorial%20Level%20Expert%20-%20NLP103.ipynb)__\n",
        "- __[Binary Classification Tutorial (CLF101) - Level Beginner](https://github.com/pycaret/pycaret/blob/master/Tutorials/Binary%20Classification%20Tutorial%20Level%20Beginner%20-%20%20CLF101.ipynb)__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB7wNDlAUwP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "01594899-46e7-4855-f6e9-3fced3f4f0c1"
      },
      "source": [
        "from pycaret.utils import enable_colab\n",
        "enable_colab()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ef4f479a3b70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycaret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menable_colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menable_colab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycaret'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsXTb_lBo2Vw",
        "outputId": "913b01ce-c86a-4baa-c769-05a6c69c7e1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pycaret\r\n",
        "from pycaret.utils import enable_colab\r\n",
        "enable_colab()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycaret\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/7b/70e41d8aa900ed47e0e2ac6a8f5cbaf9e359efdf8ae10bf89502c14ce3ed/pycaret-2.2.3-py3-none-any.whl (249kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20kB 22.3MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 40kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 51kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 61kB 16.2MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 71kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 81kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 92kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 102kB 11.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 112kB 11.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 122kB 11.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 133kB 11.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 143kB 11.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 153kB 11.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 163kB 11.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 174kB 11.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 184kB 11.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 194kB 11.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 204kB 11.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 215kB 11.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 225kB 11.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 235kB 11.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 245kB 11.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from pycaret) (2.2.4)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.6/dist-packages (from pycaret) (0.4.6)\n",
            "Collecting scikit-learn==0.23.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 21.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from pycaret) (1.5.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (from pycaret) (0.15.3)\n",
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from pycaret) (7.6.3)\n",
            "Collecting lightgbm>=2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/cd/2b7783e8c250f8191b72e9a0010e0429a799d3305c27764d7bf113dfd078/lightgbm-3.1.1-py2.py3-none-manylinux1_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from pycaret) (3.2.5)\n",
            "Collecting imbalanced-learn>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/81/8db4d87b03b998fda7c6f835d807c9ae4e3b141f978597b8d7f31600be15/imbalanced_learn-0.7.0-py3-none-any.whl (167kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 39.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pycaret) (1.1.5)\n",
            "Requirement already satisfied: plotly>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from pycaret) (4.4.1)\n",
            "Collecting kmodes>=0.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/55/d8ec1ae1f7e1e202a8a4184c6852a3ee993b202b0459672c699d0ac18fc8/kmodes-0.10.2-py2.py3-none-any.whl\n",
            "Collecting scikit-plot\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/47/32520e259340c140a4ad27c1b97050dd3254fdc517b1d59974d47037510e/scikit_plot-0.3.7-py3-none-any.whl\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.6/dist-packages (from pycaret) (0.14.0)\n",
            "Collecting pyod\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/26/ae4f4143643d2d87fe4bbc356064dd95eed95227b879ba3ee6c4e4ee81ca/pyod-0.8.6.tar.gz (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pycaret) (3.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pycaret) (1.0.0)\n",
            "Collecting yellowbrick>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/bb/57fd86c319a43666fe447bb1bc5af66fb0eb89dc4efc305a7544d50f52d6/yellowbrick-1.2.1-py3-none-any.whl (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 36.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from pycaret) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from pycaret) (1.19.5)\n",
            "Requirement already satisfied: cufflinks>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pycaret) (0.17.3)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from pycaret) (3.6.0)\n",
            "Collecting pandas-profiling>=2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/8e/645ad7f304dd8d6d7181d22d4bd3d6356331c80c2944a25be3ebe617ec38/pandas_profiling-2.10.0-py2.py3-none-any.whl (239kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 63.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: IPython in /usr/local/lib/python3.6/dist-packages (from pycaret) (5.5.0)\n",
            "Collecting mlflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/4d/a6a4460e214842377dbc43d3e83bf976d564f7976822a9351adde60af44b/mlflow-1.13.1-py3-none-any.whl (14.1MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2MB 213kB/s \n",
            "\u001b[?25hCollecting xgboost>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/84/83f58f491dce9c9aa675333dfdc833d8b3c3c031e9f651f18c9a6c4cc963/xgboost-1.3.2-py3-none-manylinux2010_x86_64.whl (157.5MB)\n",
            "\u001b[K     |████████████████████████████████| 157.5MB 101kB/s \n",
            "\u001b[?25hCollecting catboost>=0.23.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/37/bc4e0ddc30c07a96482abf1de7ed1ca54e59bba2026a33bca6d2ef286e5b/catboost-0.24.4-cp36-none-manylinux1_x86_64.whl (65.7MB)\n",
            "\u001b[K     |██████████████████████████▉     | 55.2MB 1.4MB/s eta 0:00:08"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwJ3bTCgUq_E"
      },
      "source": [
        "# 2.0 Brief Overview of Tutorial\n",
        "\n",
        "Building on from previous tutorial __[Natural Language Processing Tutorial (NLP101) - Level Beginner](https://github.com/pycaret/pycaret/blob/master/Tutorials/Natural%20Language%20Processing%20Tutorial%20Level%20Beginner%20-%20NLP101.ipynb)__ we will create a topic model in this tutorial after passing `custom_stopwords`. We will then evaluate and compare the results of topic model with the one we created in last tutorial. We will then see how to evaluate topic models using coherence value and also how to use PyCaret's unique implementation of `tune_model()` function that allows you to optimize supervised learning target (status column in this example). We will finally finish off the tutorial by using the output of topic model in `pycaret.classifcation` module to find the best classifier that can predict loan default using topic information extracted from `pycaret.nlp` module.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCU_zw7_Uq_E"
      },
      "source": [
        "# 3.0 Dataset for the Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2q7et2lUq_F"
      },
      "source": [
        "For this tutorial we will be using the same dataset that was used in __[Natural Language Processing Tutorial (NLP101) - Level Beginner](https://github.com/pycaret/pycaret/blob/master/Tutorials/Natural%20Language%20Processing%20Tutorial%20Level%20Beginner%20-%20NLP101.ipynb)__. You can download the dataset from our github repository __[(Click here to Download)](https://github.com/pycaret/pycaret/blob/master/datasets/kiva.csv)__ or you can use PyCaret's `get_data()` function to import the dataset (This will require internet connection).\n",
        "\n",
        "#### Dataset Acknowledgement:\n",
        "Kiva Microfunds https://www.kiva.org/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp4RVeGHUq_G"
      },
      "source": [
        "# 4.0 Getting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwJDbzEiUq_H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "31086655-be4f-4ae9-af6d-2c93ad9cb75c"
      },
      "source": [
        "from pycaret.datasets import get_data\n",
        "data = get_data('kiva')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-af013e88b69b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycaret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kiva'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycaret'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM6voMl1Uq_M"
      },
      "source": [
        "#check the shape of data\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLnKp6eOUq_Q"
      },
      "source": [
        "# sampling the data to select only 1000 documents\n",
        "data = data.sample(1000, random_state=786).reset_index(drop=True)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDQqkqdJUq_U"
      },
      "source": [
        "# 5.0 Setting up Environment in PyCaret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMAgmDqEUq_U"
      },
      "source": [
        "`setup()` function initializes the environment in pycaret and performs several text pre-processing steps that are imperative to work with NLP problems. In last tutorial, we have not passed any custom stopwords, which we will do in this tutorial using `custom_stopwords` parameter. All the custom stopwords passed below are obtained through the analysis we performed in __[Natural Language Processing Tutorial (NLP101) - Level Beginner](https://github.com/pycaret/pycaret/blob/master/Tutorials/Natural%20Language%20Processing%20Tutorial%20Level%20Beginner%20-%20NLP101.ipynb)__ (refer to section 9.1). These are the words with very high frequency in the documents. As such, this is adding more noise than information. Deciding a list of custom stopwords is a subjective decision and mostly stems from your understanding of the dataset. For example, in this dataset words like 'loan', 'income', 'business', 'usd' etc. are very obvious since we are working on a dataset with customer loans. Rest of the parameters passed in `setup()` below are same as last tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9iUJg9fUq_V"
      },
      "source": [
        "from pycaret.nlp import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "sRg2VLtfUq_Z"
      },
      "source": [
        "exp_nlp102 = setup(data = data, target = 'en', session_id = 123,\n",
        "                   custom_stopwords = ['loan', 'income', 'usd', 'many', 'also', 'make', 'business', 'buy', \n",
        "                                       'sell', 'purchase','year', 'people', 'able', 'enable', 'old', 'woman',\n",
        "                                       'child', 'school'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTlHAbhKUq_d"
      },
      "source": [
        "# 6.0 Create a Topic Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "A2ZwaNL6Uq_e"
      },
      "source": [
        "lda = create_model('lda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkQpeAMCUq_i"
      },
      "source": [
        "plot_model(lda, plot = 'topic_distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYaMpa-Uq_m"
      },
      "source": [
        "If you compare the output above with the one in section 9.4 of __[last tutorial](https://github.com/pycaret/pycaret/blob/master/Tutorials/Natural%20Language%20Processing%20Tutorial%20Level%20Beginner%20-%20NLP101.ipynb)__ you would notice that distribution of topics have changed, but what is more important to observe here is when you hover over the bars, keywords gives you better idea of theme of the topic in this experiment compared to last one because we have removed some noice by removing custom stopwords. For eg. `Topic 3` seems to be about customers seeking trade loans as it include keywords like 'hair', 'salon', 'wood', 'machine'.  `Topic 0` is about farming/agricultural loans, `Topic 1` is mostly about retail loans and `Topic 2` seems to be about loans for domestic reasons. \n",
        "\n",
        "Topic Modeling is very iterative machine learning task, finding the right list of custom stopwords is only possible after several iterations. We encourage you to repeat the experiments to gain actionable insights that finally leads you to the best working and implementable model. So far we have learned how to create and analyze a topic model using `pycaret.nlp` module. In next section we will go a few steps deeper to understand how to evaluate a topic model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMEAQiJSUq_m"
      },
      "source": [
        "# 7.0 Evaluating Topic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serdV_2xUq_n"
      },
      "source": [
        "Many topic models including Latent Dirichlet allocation are probabilistic models, providing both a predictive and latent topic representation. It is generally assumed that results generated by these models are meaningful and useful and due to its unsupervised training process it is hard to evaluate those assumptions. Nevertheless, it is equally important to identify if a trained model is objectively good or bad, as well have an ability to compare different models/methods. To do so, one would require an objective measure for the quality. Traditionally, and still for many practical applications, to evaluate if \"the correct thing\" has been learned about the corpus, an implicit knowledge and \"eyeballing\" approaches are used. Ideally, we’d like to capture this information in a single metric that can be maximized, and compared. The approaches that are commonly used today:\n",
        "\n",
        "- **Eye Balling Models :** Look at Top N words, Topics / Documents etc. \n",
        "- **Intrinsic Evaluation Metrics:** Interpretability and semantics of model\n",
        "- **Extrinsic Evaluation Metrics:** Is model good at performing predefined tasks, such as classification (later in this tutorial we will use our topic model to build a classifier to predict loan default)\n",
        "- **Human Judgements:** Does the topic model improves your understanding of the problem?\n",
        "\n",
        "In this section we will learn how to evaluate coherence value of a topic model using `tune_model()` function. Followed by extrinsic evaluation on number of topics in a topic model to optimize classifier that can predict default using `status` column in the dataset.\n",
        "\n",
        "__[Read More about Model Evaluation](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-8nZHWQUq_o"
      },
      "source": [
        "### 7.1 Intrinsic Evaluation using Coherence Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp-85ZVTUq_o"
      },
      "source": [
        "**What is Topic Coherence?** Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. `tune_model()` function iterates on a pre-defined grid with different number of topics and create a model for each parameter. Topic coherence is then evaluated for different models and are visually presented in a graph the has `Coherence Score` on y-axis as a function of `# Topics` on x-axis. See example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By8vrcSOUq_p"
      },
      "source": [
        "tuned_unsupervised = tune_model(model = 'lda', multi_core = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VxhUYjAUq_t"
      },
      "source": [
        "Model with highest coherence score is the best model based on intrinsic evaluation criteria. As appealing as it may sound that performance of a topic model can be captured in one number i.e. Coherence Score, it doesn't come without its downside. We encourage you to do some more reading about Coherence Score to understand more about it. __[[Read More]](https://www.aclweb.org/anthology/D12-1087.pdf)__\n",
        "\n",
        "We have only covered Coherence in this tutorial. The other popular measure is perplexity. It captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set. Focussing on the log-likelihood part, you can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data. However, recent studies have shown that predictive likelihood (or equivalently, perplexity) and human judgment are often not correlated, and even sometimes slightly anti-correlated as such optimizing for perplexity may not yield human interpretable topics. __[[Reference]](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfIj89XZUq_u"
      },
      "source": [
        "### 7.2 Extrinsic Evaluation using Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCkrOKPAUq_w"
      },
      "source": [
        "The dataset we are using is labelled using `status` column (1 means loan default, 0 means no default). We will now use `tune_model()` function to determine the best number of topics. Best in this case is defined by the measure of interest in supervised machine learning which in this case is `Accuracy` since this is a classification problem. See example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgv9vS3OUq_x"
      },
      "source": [
        "tuned_classification = tune_model(model = 'lda', multi_core = True, supervised_target = 'status')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEUsK-1dUq_2"
      },
      "source": [
        "In this example the `Accuracy` is optimized when `num_topics` are set to `4`. It is very likely that number of topics to optimize the supervised metric such as accuracy in this case would be different than model with best coherence value. At the end of the day, which one to use is totally dependent on the ues case of topic model. Evaluating topic models is a complex subject. It is unlikely that you will understand them fully if this is your first time doing topic modeling. We recommend you to watch this __[YouTube Video](https://www.youtube.com/watch?v=UkmIljRIG_M)__, if you interested in learning more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdkFJMwVUq_2"
      },
      "source": [
        "# 8.0 Saving the experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQYMwSbfUq_3"
      },
      "source": [
        "In __[Natural Language Processing Tutorial (NLP101) - Level Beginner](https://github.com/pycaret/pycaret/blob/master/Tutorials/Natural%20Language%20Processing%20Tutorial%20Level%20Beginner%20-%20NLP101.ipynb)__ we have learned how to save and load the model. In this experiment we will learn how to save the entire experiment including all the outputs and models we have built in this experiment. Saving experiment is as simple as saving model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAvTEIrTUq_3"
      },
      "source": [
        "save_experiment('Experiment_123 08Feb2020')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3A9CnvoUq_7"
      },
      "source": [
        "# 9.0 Loading saved experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sulVyT8LUq_7"
      },
      "source": [
        "To load a saved experiment on a future date or in a different environment, we would use the `load_experiment()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SnUkCWVUq_8"
      },
      "source": [
        "saved_experiment = load_experiment('Experiment_123 08Feb2020')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfutxCCqUrAA"
      },
      "source": [
        "Notice that when you used `load_experiment()`, it has loaded the entire experiments and all the intermediate outputs in variable `saved_experiment`. You can access specific items in a similar way you would access list elements in Python. See example below in which we are accessing our final stacking ensembler and store it in `final_stack_soft_loaded` variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67aqRRcbUrAB"
      },
      "source": [
        "saved_lda = saved_experiment[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9QlpgPKUrAE"
      },
      "source": [
        "print(saved_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqUO2XM2UrAI"
      },
      "source": [
        "# 10.0 Wrap-up / Next Steps?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8RfU_qAUrAJ"
      },
      "source": [
        "We have covered several key concepts in this tutorial such as model evaluation using intrinsic and extrinsic technique. In next tutorial. We have performed several text pre-processing steps including removal of custom_stopwords using `setup()` then we have created a topic model and compared the results with the one we created in last tutorial. We have also talked about different ways to evaluate topic model and have used `tune_model()` to evaluate coherence value of a LDA model. We have also used `tune_model()` to evaluate number of topics in a supervised setting (in this case we have used it to build a classifier to predict loan status).\n",
        "\n",
        "In next tutorial we will use `pycaret.nlp` together with `pycaret.classification` and focus on using supervised and unsupervised module of pycaret together. \n",
        "\n",
        "See you at the next tutorial. Follow the link to __[Natural Language Processing (NLP103) - Level Expert](https://github.com/pycaret/pycaret/blob/master/Tutorials/Natural%20Language%20Processing%20Tutorial%20Level%20Expert%20-%20NLP103.ipynb)__"
      ]
    }
  ]
}